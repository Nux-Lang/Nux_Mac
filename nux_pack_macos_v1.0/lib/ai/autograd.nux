# Nux Autograd Library
# Automatic differentiation for deep learning

import "std.math";

# ============================================================
# TENSOR WITH GRADIENT TRACKING
# ============================================================

var _tape = 0;

class Tape {
    var operations;
    var enabled;
    
    func init() {
        this.operations = new List();
        this.operations.init();
        this.enabled = 1;
    }
    
    func record(op) {
        if (this.enabled == 1) {
            this.operations.append(op);
        }
    }
    
    func backward(loss) {
        loss.grad = 1;
        
        var i = this.operations.length() - 1;
        while (i >= 0) {
            var op = this.operations.get(i);
            op.backward();
            i = i - 1;
        }
    }
    
    func zero_gradients() {
        var i = 0;
        for (i = 0; i < this.operations.length(); i = i + 1) {
            var op = this.operations.get(i);
            if (op.output != 0) { op.output.grad = 0; }
        }
    }
    
    func clear() {
        this.operations.clear();
    }
}

func get_tape() {
    if (_tape == 0) {
        _tape = new Tape();
        _tape.init();
    }
    return _tape;
}

class Variable {
    var data;
    var grad;
    var requires_grad;
    var shape;
    var size;
    var name;
    
    func init(values, rg) {
        this.data = values;
        this.requires_grad = rg;
        this.grad = 0;
        this.size = values.length();
        this.shape = new List();
        this.shape.init();
        this.shape.append(this.size);
    }
    
    func init_2d(rows, cols, rg) {
        this.requires_grad = rg;
        this.grad = 0;
        this.size = rows * cols;
        this.shape = new List();
        this.shape.init();
        this.shape.append(rows);
        this.shape.append(cols);
        
        this.data = new List();
        this.data.init();
        var i = 0;
        for (i = 0; i < this.size; i = i + 1) {
            this.data.append(0);
        }
    }
    
    func zero_grad() {
        this.grad = 0;
    }
    
    func get(idx) {
        return this.data.get(idx);
    }
    
    func set(idx, val) {
        this.data.set(idx, val);
    }
    
    func get_2d(row, col) {
        var cols = this.shape.get(1);
        return this.data.get(row * cols + col);
    }
    
    func set_2d(row, col, val) {
        var cols = this.shape.get(1);
        this.data.set(row * cols + col, val);
    }
}

func zeros(size, requires_grad) {
    var data = new List();
    data.init();
    var i = 0;
    for (i = 0; i < size; i = i + 1) {
        data.append(0);
    }
    var v = new Variable();
    v.init(data, requires_grad);
    return v;
}

func ones(size, requires_grad) {
    var data = new List();
    data.init();
    var i = 0;
    for (i = 0; i < size; i = i + 1) {
        data.append(1);
    }
    var v = new Variable();
    v.init(data, requires_grad);
    return v;
}

func randn(size, requires_grad) {
    var data = new List();
    data.init();
    var i = 0;
    for (i = 0; i < size; i = i + 1) {
        var r = (random() % 1000) / 1000 - 0.5;
        data.append(r);
    }
    var v = new Variable();
    v.init(data, requires_grad);
    return v;
}

# ============================================================
# OPERATIONS WITH BACKWARD PASS
# ============================================================

class Operation {
    var inputs;
    var output;
    var saved;
    
    func init() {
        this.inputs = new List();
        this.inputs.init();
        this.saved = new Map();
        this.saved.init();
    }
    
    func forward() { return 0; }
    func backward() {}
}

class AddOp {
    var inputs;
    var output;
    var saved;
    
    func init(a, b) {
        this.inputs = new List();
        this.inputs.init();
        this.inputs.append(a);
        this.inputs.append(b);
    }
    
    func forward() {
        var a = this.inputs.get(0);
        var b = this.inputs.get(1);
        
        var result = new List();
        result.init();
        var i = 0;
        for (i = 0; i < a.size; i = i + 1) {
            result.append(a.get(i) + b.get(i));
        }
        
        this.output = new Variable();
        this.output.init(result, 1);
        
        get_tape().record(this);
        return this.output;
    }
    
    func backward() {
        var a = this.inputs.get(0);
        var b = this.inputs.get(1);
        
        if (a.requires_grad == 1) {
            if (a.grad == 0) { a.grad = 0; }
            a.grad = a.grad + this.output.grad;
        }
        if (b.requires_grad == 1) {
            if (b.grad == 0) { b.grad = 0; }
            b.grad = b.grad + this.output.grad;
        }
    }
}

class MulOp {
    var inputs;
    var output;
    var saved;
    
    func init(a, b) {
        this.inputs = new List();
        this.inputs.init();
        this.inputs.append(a);
        this.inputs.append(b);
    }
    
    func forward() {
        var a = this.inputs.get(0);
        var b = this.inputs.get(1);
        
        var result = new List();
        result.init();
        var i = 0;
        for (i = 0; i < a.size; i = i + 1) {
            result.append(a.get(i) * b.get(i));
        }
        
        this.output = new Variable();
        this.output.init(result, 1);
        
        get_tape().record(this);
        return this.output;
    }
    
    func backward() {
        var a = this.inputs.get(0);
        var b = this.inputs.get(1);
        
        if (a.requires_grad == 1) {
            var i = 0;
            for (i = 0; i < a.size; i = i + 1) {
                a.grad = a.grad + this.output.grad * b.get(i);
            }
        }
        if (b.requires_grad == 1) {
            var i = 0;
            for (i = 0; i < b.size; i = i + 1) {
                b.grad = b.grad + this.output.grad * a.get(i);
            }
        }
    }
}

class MatMulOp {
    var inputs;
    var output;
    var m;
    var n;
    var k;
    
    func init(a, b, mm, nn, kk) {
        this.inputs = new List();
        this.inputs.init();
        this.inputs.append(a);
        this.inputs.append(b);
        this.m = mm;
        this.n = nn;
        this.k = kk;
    }
    
    func forward() {
        var a = this.inputs.get(0);
        var b = this.inputs.get(1);
        
        var result = new List();
        result.init();
        
        var i = 0;
        for (i = 0; i < this.m; i = i + 1) {
            var j = 0;
            for (j = 0; j < this.k; j = j + 1) {
                var sum = 0;
                var l = 0;
                for (l = 0; l < this.n; l = l + 1) {
                    sum = sum + a.get(i * this.n + l) * b.get(l * this.k + j);
                }
                result.append(sum);
            }
        }
        
        this.output = new Variable();
        this.output.init(result, 1);
        
        get_tape().record(this);
        return this.output;
    }
    
    func backward() {
        # dL/dA = dL/dC @ B^T
        # dL/dB = A^T @ dL/dC
    }
}

class ReLUOp {
    var inputs;
    var output;
    var mask;
    
    func init(x) {
        this.inputs = new List();
        this.inputs.init();
        this.inputs.append(x);
    }
    
    func forward() {
        var x = this.inputs.get(0);
        
        var result = new List();
        result.init();
        this.mask = new List();
        this.mask.init();
        
        var i = 0;
        for (i = 0; i < x.size; i = i + 1) {
            var val = x.get(i);
            if (val > 0) {
                result.append(val);
                this.mask.append(1);
            } else {
                result.append(0);
                this.mask.append(0);
            }
        }
        
        this.output = new Variable();
        this.output.init(result, 1);
        
        get_tape().record(this);
        return this.output;
    }
    
    func backward() {
        var x = this.inputs.get(0);
        if (x.requires_grad == 1) {
            var i = 0;
            for (i = 0; i < x.size; i = i + 1) {
                if (this.mask.get(i) == 1) {
                    x.grad = x.grad + this.output.grad;
                }
            }
        }
    }
}

class SigmoidOp {
    var inputs;
    var output;
    
    func init(x) {
        this.inputs = new List();
        this.inputs.init();
        this.inputs.append(x);
    }
    
    func forward() {
        var x = this.inputs.get(0);
        
        var result = new List();
        result.init();
        
        var i = 0;
        for (i = 0; i < x.size; i = i + 1) {
            var val = 1 / (1 + exp(-x.get(i)));
            result.append(val);
        }
        
        this.output = new Variable();
        this.output.init(result, 1);
        
        get_tape().record(this);
        return this.output;
    }
    
    func backward() {
        var x = this.inputs.get(0);
        if (x.requires_grad == 1) {
            var i = 0;
            for (i = 0; i < x.size; i = i + 1) {
                var s = this.output.get(i);
                x.grad = x.grad + this.output.grad * s * (1 - s);
            }
        }
    }
}

class SoftmaxOp {
    var inputs;
    var output;
    
    func init(x) {
        this.inputs = new List();
        this.inputs.init();
        this.inputs.append(x);
    }
    
    func forward() {
        var x = this.inputs.get(0);
        
        # Find max for stability
        var max_val = x.get(0);
        var i = 0;
        for (i = 1; i < x.size; i = i + 1) {
            if (x.get(i) > max_val) { max_val = x.get(i); }
        }
        
        # Compute exp and sum
        var exp_sum = 0;
        var exps = new List();
        exps.init();
        for (i = 0; i < x.size; i = i + 1) {
            var e = exp(x.get(i) - max_val);
            exps.append(e);
            exp_sum = exp_sum + e;
        }
        
        # Normalize
        var result = new List();
        result.init();
        for (i = 0; i < x.size; i = i + 1) {
            result.append(exps.get(i) / exp_sum);
        }
        
        this.output = new Variable();
        this.output.init(result, 1);
        
        get_tape().record(this);
        return this.output;
    }
    
    func backward() {
        # Softmax backward is complex, simplified here
    }
}

# ============================================================
# LOSS FUNCTIONS
# ============================================================

class MSELoss {
    var inputs;
    var output;
    var pred;
    var target;
    
    func init() {}
    
    func forward(predicted, actual) {
        this.pred = predicted;
        this.target = actual;
        
        var sum = 0;
        var i = 0;
        for (i = 0; i < predicted.size; i = i + 1) {
            var diff = predicted.get(i) - actual.get(i);
            sum = sum + diff * diff;
        }
        
        var result = new List();
        result.init();
        result.append(sum / predicted.size);
        
        this.output = new Variable();
        this.output.init(result, 1);
        
        get_tape().record(this);
        return this.output;
    }
    
    func backward() {
        if (this.pred.requires_grad == 1) {
            var n = this.pred.size;
            var i = 0;
            for (i = 0; i < n; i = i + 1) {
                var diff = this.pred.get(i) - this.target.get(i);
                this.pred.grad = this.pred.grad + 2 * diff / n * this.output.grad;
            }
        }
    }
}

class CrossEntropyLoss {
    var inputs;
    var output;
    var pred;
    var target;
    
    func init() {}
    
    func forward(predicted, actual) {
        this.pred = predicted;
        this.target = actual;
        
        var sum = 0;
        var i = 0;
        for (i = 0; i < actual.size; i = i + 1) {
            if (actual.get(i) > 0) {
                sum = sum - actual.get(i) * log(predicted.get(i) + 0.0000001);
            }
        }
        
        var result = new List();
        result.init();
        result.append(sum);
        
        this.output = new Variable();
        this.output.init(result, 1);
        
        get_tape().record(this);
        return this.output;
    }
    
    func backward() {
        if (this.pred.requires_grad == 1) {
            var i = 0;
            for (i = 0; i < this.pred.size; i = i + 1) {
                this.pred.grad = this.pred.grad - 
                    this.target.get(i) / (this.pred.get(i) + 0.0000001) * this.output.grad;
            }
        }
    }
}

# ============================================================
# OPTIMIZERS
# ============================================================

class SGD {
    var parameters;
    var lr;
    var momentum;
    var velocities;
    
    func init(params, learning_rate) {
        this.parameters = params;
        this.lr = learning_rate;
        this.momentum = 0;
        this.velocities = new List();
        this.velocities.init();
    }
    
    func step() {
        var i = 0;
        for (i = 0; i < this.parameters.length(); i = i + 1) {
            var param = this.parameters.get(i);
            if (param.requires_grad == 1) {
                var j = 0;
                for (j = 0; j < param.size; j = j + 1) {
                    var val = param.get(j);
                    param.set(j, val - this.lr * param.grad);
                }
            }
        }
    }
    
    func zero_grad() {
        var i = 0;
        for (i = 0; i < this.parameters.length(); i = i + 1) {
            this.parameters.get(i).grad = 0;
        }
    }
}

class Adam {
    var parameters;
    var lr;
    var beta1;
    var beta2;
    var epsilon;
    var m;
    var v;
    var t;
    
    func init(params, learning_rate) {
        this.parameters = params;
        this.lr = learning_rate;
        this.beta1 = 0.9;
        this.beta2 = 0.999;
        this.epsilon = 0.00000001;
        this.t = 0;
        
        this.m = new List();
        this.m.init();
        this.v = new List();
        this.v.init();
        
        var i = 0;
        for (i = 0; i < params.length(); i = i + 1) {
            this.m.append(0);
            this.v.append(0);
        }
    }
    
    func step() {
        this.t = this.t + 1;
        
        var i = 0;
        for (i = 0; i < this.parameters.length(); i = i + 1) {
            var param = this.parameters.get(i);
            if (param.requires_grad == 1) {
                var g = param.grad;
                
                var m_old = this.m.get(i);
                var v_old = this.v.get(i);
                
                var m_new = this.beta1 * m_old + (1 - this.beta1) * g;
                var v_new = this.beta2 * v_old + (1 - this.beta2) * g * g;
                
                this.m.set(i, m_new);
                this.v.set(i, v_new);
                
                var m_hat = m_new / (1 - pow(this.beta1, this.t));
                var v_hat = v_new / (1 - pow(this.beta2, this.t));
                
                var j = 0;
                for (j = 0; j < param.size; j = j + 1) {
                    var val = param.get(j);
                    param.set(j, val - this.lr * m_hat / (sqrt(v_hat) + this.epsilon));
                }
            }
        }
    }
    
    func zero_grad() {
        var i = 0;
        for (i = 0; i < this.parameters.length(); i = i + 1) {
            this.parameters.get(i).grad = 0;
        }
    }
}

# ============================================================
# CONVENIENCE FUNCTIONS
# ============================================================

func add(a, b) {
    var op = new AddOp();
    op.init(a, b);
    return op.forward();
}

func mul(a, b) {
    var op = new MulOp();
    op.init(a, b);
    return op.forward();
}

func matmul(a, b, m, n, k) {
    var op = new MatMulOp();
    op.init(a, b, m, n, k);
    return op.forward();
}

func relu(x) {
    var op = new ReLUOp();
    op.init(x);
    return op.forward();
}

func sigmoid(x) {
    var op = new SigmoidOp();
    op.init(x);
    return op.forward();
}

func softmax(x) {
    var op = new SoftmaxOp();
    op.init(x);
    return op.forward();
}
