// Nux Standard Library - Stream Processing
// High-performance data stream processing and ETL

// ===== STREAM =====

fn stream_create(source) {
    // Create stream
    return {
        source: source,
        operations: []
    };
}

fn stream_of(...values) {
    // Create stream from values
    return stream_create(values);
}

fn stream_range(start, end) {
    // Create stream from range
    let values = [];
    let i = start;
    
    while (i < end) {
        arr_push(values, i);
        i = i + 1;
    }
    
    return stream_create(values);
}

fn stream_generate(generator, count) {
    // Generate stream
    let values = [];
    let i = 0;
    
    while (i < count) {
        arr_push(values, generator(i));
        i = i + 1;
    }
    
    return stream_create(values);
}

// ===== TRANSFORMATION OPERATIONS =====

fn stream_map(stream, fn) {
    // Map transformation
    arr_push(stream.operations, {
        type: "map",
        fn: fn
    });
    return stream;
}

fn stream_filter(stream, predicate) {
    // Filter operation
    arr_push(stream.operations, {
        type: "filter",
        fn: predicate
    });
    return stream;
}

fn stream_flatMap(stream, fn) {
    // Flat map operation
    arr_push(stream.operations, {
        type: "flatMap",
        fn: fn
    });
    return stream;
}

fn stream_distinct(stream) {
    // Remove duplicates
    arr_push(stream.operations, {
        type: "distinct"
    });
    return stream;
}

fn stream_sorted(stream, comparator) {
    // Sort stream
    arr_push(stream.operations, {
        type: "sorted",
        fn: comparator
    });
    return stream;
}

// ===== AGGREGATION OPERATIONS =====

fn stream_reduce(stream, reducer, initial) {
    // Reduce stream
    let data = stream_execute(stream);
    return arr_reduce(data, reducer, initial);
}

fn stream_count(stream) {
    // Count elements
    let data = stream_execute(stream);
    return arr_length(data);
}

fn stream_sum(stream) {
    // Sum elements
    return stream_reduce(stream, fn(acc, x) { return acc + x; }, 0);
}

fn stream_average(stream) {
    // Average elements
    let data = stream_execute(stream);
    return arr_sum(data) / arr_length(data);
}

fn stream_min(stream) {
    // Minimum element
    let data = stream_execute(stream);
    return arr_min(data);
}

fn stream_max(stream) {
    // Maximum element
    let data = stream_execute(stream);
    return arr_max(data);
}

// ===== COLLECTION OPERATIONS =====

fn stream_toArray(stream) {
    // Collect to array
    return stream_execute(stream);
}

fn stream_toSet(stream) {
    // Collect to set
    let data = stream_execute(stream);
    let set = set_create();
    
    arr_forEach(data, fn(item) {
        set_add(set, item);
    });
    
    return set;
}

fn stream_groupBy(stream, keyFn) {
    // Group by key
    let data = stream_execute(stream);
    return groupBy(data, keyFn);
}

fn stream_partition(stream, predicate) {
    // Partition stream
    let data = stream_execute(stream);
    return partition(data, predicate);
}

// ===== EXECUTION =====

fn stream_execute(stream) {
    // Execute stream pipeline
    let data = stream.source;
    
    // Apply operations
    arr_forEach(stream.operations, fn(op) {
        if (op.type == "map") {
            data = arr_map(data, op.fn);
        } else if (op.type == "filter") {
            data = arr_filter(data, op.fn);
        } else if (op.type == "flatMap") {
            data = arr_flatMap(data, op.fn);
        } else if (op.type == "distinct") {
            data = arr_unique(data);
        } else if (op.type == "sorted") {
            data = arr_sort(data, op.fn);
        }
    });
    
    return data;
}

// ===== PARALLEL STREAM =====

fn parallelStream_create(source, numThreads) {
    // Create parallel stream
    return {
        source: source,
        operations: [],
        numThreads: numThreads || 4,
        parallel: true
    };
}

fn parallelStream_execute(stream) {
    // Execute in parallel
    let chunkSize = ceil(arr_length(stream.source) / stream.numThreads);
    let chunks = chunk(stream.source, chunkSize);
    
    // Process chunks in parallel
    let results = arr_map_parallel(chunks, fn(chunk) {
        let data = chunk;
        
        arr_forEach(stream.operations, fn(op) {
            if (op.type == "map") {
                data = arr_map(data, op.fn);
            } else if (op.type == "filter") {
                data = arr_filter(data, op.fn);
            }
        });
        
        return data;
    });
    
    // Merge results
    return arr_flatten(results);
}

// ===== WINDOWING =====

fn stream_window(stream, size, slide) {
    // Sliding window
    arr_push(stream.operations, {
        type: "window",
        size: size,
        slide: slide || size
    });
    return stream;
}

fn stream_tumbling(stream, size) {
    // Tumbling window
    return stream_window(stream, size, size);
}

fn stream_sliding(stream, size, slide) {
    // Sliding window
    return stream_window(stream, size, slide);
}

// ===== TIME-BASED OPERATIONS =====

fn stream_buffer(stream, duration) {
    // Buffer for duration
    arr_push(stream.operations, {
        type: "buffer",
        duration: duration
    });
    return stream;
}

fn stream_sample(stream, interval) {
    // Sample at interval
    arr_push(stream.operations, {
        type: "sample",
        interval: interval
    });
    return stream;
}

// ===== ETL OPERATIONS =====

fn etl_extract(source, format) {
    // Extract data
    if (format == "csv") {
        return csv_parse(source);
    } else if (format == "json") {
        return json_parse(source);
    } else if (format == "xml") {
        return xml_parse(source);
    }
    
    return source;
}

fn etl_transform(data, transformations) {
    // Transform data
    let stream = stream_create(data);
    
    arr_forEach(transformations, fn(transform) {
        if (transform.type == "map") {
            stream = stream_map(stream, transform.fn);
        } else if (transform.type == "filter") {
            stream = stream_filter(stream, transform.fn);
        } else if (transform.type == "aggregate") {
            stream = stream_groupBy(stream, transform.keyFn);
        }
    });
    
    return stream_toArray(stream);
}

fn etl_load(data, destination, format) {
    // Load data
    if (format == "csv") {
        return csv_stringify(data);
    } else if (format == "json") {
        return json_stringify(data);
    } else if (format == "database") {
        // Insert into database
        return db_insertBatch(destination, data);
    }
    
    return data;
}

fn etl_pipeline(config) {
    // Complete ETL pipeline
    let data = etl_extract(config.source, config.sourceFormat);
    data = etl_transform(data, config.transformations);
    return etl_load(data, config.destination, config.destFormat);
}

// ===== DATA VALIDATION =====

fn validate_schema(data, schema) {
    // Validate data against schema
    let errors = [];
    
    arr_forEach(data, fn(item) {
        let keys = obj_keys(schema);
        
        arr_forEach(keys, fn(key) {
            let rule = schema[key];
            
            if (rule.required && !obj_has(item, key)) {
                arr_push(errors, "Missing required field: " + key);
            }
            
            if (obj_has(item, key) && rule.type) {
                let actualType = typeof(item[key]);
                if (actualType != rule.type) {
                    arr_push(errors, "Type mismatch for " + key);
                }
            }
        });
    });
    
    return {
        valid: arr_length(errors) == 0,
        errors: errors
    };
}

// ===== DATA QUALITY =====

fn quality_checkCompleteness(data, requiredFields) {
    // Check data completeness
    let complete = 0;
    let total = arr_length(data);
    
    arr_forEach(data, fn(item) {
        let hasAll = arr_every(requiredFields, fn(field) {
            return obj_has(item, field) && item[field] != null;
        });
        
        if (hasAll) {
            complete = complete + 1;
        }
    });
    
    return complete / total;
}

fn quality_checkUniqueness(data, keyField) {
    // Check uniqueness
    let seen = set_create();
    let duplicates = 0;
    
    arr_forEach(data, fn(item) {
        let key = item[keyField];
        
        if (set_has(seen, key)) {
            duplicates = duplicates + 1;
        } else {
            set_add(seen, key);
        }
    });
    
    return 1 - (duplicates / arr_length(data));
}

fn quality_checkConsistency(data, rules) {
    // Check data consistency
    let violations = 0;
    
    arr_forEach(data, fn(item) {
        arr_forEach(rules, fn(rule) {
            if (!rule.check(item)) {
                violations = violations + 1;
            }
        });
    });
    
    return 1 - (violations / (arr_length(data) * arr_length(rules)));
}
