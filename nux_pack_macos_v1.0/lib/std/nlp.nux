// Nux Standard Library - Natural Language Processing
// Text analysis, tokenization, and NLP algorithms

// ===== TOKENIZATION =====

fn nlp_tokenize(text) {
    // Split text into words
    let words = [];
    let current = "";
    let i = 0;
    
    while (i < length(text)) {
        let char = text[i];
        
        if (char == " " || char == "\n" || char == "\t" || char == "." || char == "," || char == "!" || char == "?") {
            if (length(current) > 0) {
                arr_push(words, to_lower(current));
                current = "";
            }
        } else {
            current = current + char;
        }
        
        i = i + 1;
    }
    
    if (length(current) > 0) {
        arr_push(words, to_lower(current));
    }
    
    return words;
}

fn nlp_tokenizeSentences(text) {
    // Split text into sentences
    let sentences = [];
    let current = "";
    let i = 0;
    
    while (i < length(text)) {
        let char = text[i];
        current = current + char;
        
        if (char == "." || char == "!" || char == "?") {
            arr_push(sentences, trim(current));
            current = "";
        }
        
        i = i + 1;
    }
    
    if (length(trim(current)) > 0) {
        arr_push(sentences, trim(current));
    }
    
    return sentences;
}

// ===== STOPWORDS =====

fn nlp_getStopwords() {
    // Common English stopwords
    return ["the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for", 
            "of", "with", "by", "from", "as", "is", "was", "are", "were", "be", 
            "been", "being", "have", "has", "had", "do", "does", "did", "will", 
            "would", "could", "should", "may", "might", "can", "this", "that", 
            "these", "those", "i", "you", "he", "she", "it", "we", "they"];
}

fn nlp_removeStopwords(words) {
    // Remove stopwords from word list
    let stopwords = nlp_getStopwords();
    return arr_filter(words, fn(word) {
        return !arr_contains(stopwords, word);
    });
}

// ===== STEMMING =====

fn nlp_stem(word) {
    // Simple Porter stemmer (simplified)
    let len = length(word);
    
    // Remove common suffixes
    if (endsWith(word, "ing") && len > 5) {
        return substring(word, 0, len - 3);
    }
    if (endsWith(word, "ed") && len > 4) {
        return substring(word, 0, len - 2);
    }
    if (endsWith(word, "s") && len > 3) {
        return substring(word, 0, len - 1);
    }
    
    return word;
}

fn nlp_stemWords(words) {
    // Stem all words
    return arr_map(words, nlp_stem);
}

// ===== FREQUENCY ANALYSIS =====

fn nlp_wordFrequency(words) {
    // Calculate word frequencies
    let freq = {};
    let i = 0;
    
    while (i < arr_length(words)) {
        let word = words[i];
        if (obj_has(freq, word)) {
            freq[word] = freq[word] + 1;
        } else {
            freq[word] = 1;
        }
        i = i + 1;
    }
    
    return freq;
}

fn nlp_topWords(words, n) {
    // Get top N most frequent words
    let freq = nlp_wordFrequency(words);
    let entries = obj_entries(freq);
    
    // Sort by frequency
    let sorted = arr_sort(entries, fn(a, b) {
        return b[1] - a[1];
    });
    
    return take(sorted, n);
}

// ===== TF-IDF =====

fn nlp_tf(word, document) {
    // Term frequency
    let count = 0;
    let i = 0;
    
    while (i < arr_length(document)) {
        if (document[i] == word) {
            count = count + 1;
        }
        i = i + 1;
    }
    
    return count / arr_length(document);
}

fn nlp_idf(word, documents) {
    // Inverse document frequency
    let docCount = 0;
    let i = 0;
    
    while (i < arr_length(documents)) {
        if (arr_contains(documents[i], word)) {
            docCount = docCount + 1;
        }
        i = i + 1;
    }
    
    return log(arr_length(documents) / (docCount + 1));
}

fn nlp_tfidf(word, document, documents) {
    // TF-IDF score
    return nlp_tf(word, document) * nlp_idf(word, documents);
}

// ===== N-GRAMS =====

fn nlp_ngrams(words, n) {
    // Generate n-grams
    let ngrams = [];
    let i = 0;
    
    while (i <= arr_length(words) - n) {
        let ngram = arr_slice(words, i, i + n);
        arr_push(ngrams, arr_join(ngram, " "));
        i = i + 1;
    }
    
    return ngrams;
}

fn nlp_bigrams(words) {
    // Generate bigrams
    return nlp_ngrams(words, 2);
}

fn nlp_trigrams(words) {
    // Generate trigrams
    return nlp_ngrams(words, 3);
}

// ===== SIMILARITY =====

fn nlp_jaccardSimilarity(set1, set2) {
    // Jaccard similarity between two sets
    let intersection = set_intersection(set1, set2);
    let union = set_union(set1, set2);
    
    return set_size(intersection) / set_size(union);
}

fn nlp_cosineSimilarity(vec1, vec2) {
    // Cosine similarity between two vectors
    let dotProduct = 0;
    let mag1 = 0;
    let mag2 = 0;
    let i = 0;
    
    while (i < arr_length(vec1)) {
        dotProduct = dotProduct + vec1[i] * vec2[i];
        mag1 = mag1 + vec1[i] * vec1[i];
        mag2 = mag2 + vec2[i] * vec2[i];
        i = i + 1;
    }
    
    return dotProduct / (sqrt(mag1) * sqrt(mag2));
}

fn nlp_levenshteinDistance(str1, str2) {
    // Edit distance between two strings
    let len1 = length(str1);
    let len2 = length(str2);
    let matrix = [];
    
    // Initialize matrix
    let i = 0;
    while (i <= len1) {
        matrix[i] = [];
        let j = 0;
        while (j <= len2) {
            if (i == 0) {
                matrix[i][j] = j;
            } else if (j == 0) {
                matrix[i][j] = i;
            } else {
                matrix[i][j] = 0;
            }
            j = j + 1;
        }
        i = i + 1;
    }
    
    // Fill matrix
    i = 1;
    while (i <= len1) {
        let j = 1;
        while (j <= len2) {
            let cost = str1[i - 1] == str2[j - 1] ? 0 : 1;
            matrix[i][j] = min(
                min(matrix[i - 1][j] + 1, matrix[i][j - 1] + 1),
                matrix[i - 1][j - 1] + cost
            );
            j = j + 1;
        }
        i = i + 1;
    }
    
    return matrix[len1][len2];
}

// ===== SENTIMENT ANALYSIS =====

fn nlp_sentimentScore(text) {
    // Simple sentiment analysis
    let positive = ["good", "great", "excellent", "amazing", "wonderful", "fantastic", 
                    "love", "best", "awesome", "happy", "joy", "perfect"];
    let negative = ["bad", "terrible", "awful", "horrible", "worst", "hate", "poor", 
                    "sad", "angry", "disappointing", "useless"];
    
    let words = nlp_tokenize(text);
    let score = 0;
    let i = 0;
    
    while (i < arr_length(words)) {
        let word = words[i];
        if (arr_contains(positive, word)) {
            score = score + 1;
        } else if (arr_contains(negative, word)) {
            score = score - 1;
        }
        i = i + 1;
    }
    
    return score;
}

fn nlp_sentiment(text) {
    // Classify sentiment
    let score = nlp_sentimentScore(text);
    
    if (score > 0) {
        return "positive";
    } else if (score < 0) {
        return "negative";
    } else {
        return "neutral";
    }
}

// ===== NAMED ENTITY RECOGNITION =====

fn nlp_extractEmails(text) {
    // Extract email addresses
    return regex_extractEmails(text);
}

fn nlp_extractURLs(text) {
    // Extract URLs
    return regex_extractURLs(text);
}

fn nlp_extractNumbers(text) {
    // Extract numbers
    return regex_extractNumbers(text);
}

fn nlp_extractDates(text) {
    // Extract dates (simplified)
    let pattern = "[0-9]{1,2}/[0-9]{1,2}/[0-9]{2,4}";
    return regex_matchAll(text, pattern);
}

// ===== TEXT GENERATION =====

fn nlp_markovChain(text, order) {
    // Build Markov chain for text generation
    let words = nlp_tokenize(text);
    let chain = {};
    let i = 0;
    
    while (i < arr_length(words) - order) {
        let state = arr_join(arr_slice(words, i, i + order), " ");
        let next = words[i + order];
        
        if (!obj_has(chain, state)) {
            chain[state] = [];
        }
        
        arr_push(chain[state], next);
        i = i + 1;
    }
    
    return chain;
}

fn nlp_generateText(chain, startState, length) {
    // Generate text using Markov chain
    let result = split(startState, " ");
    let current = startState;
    let i = 0;
    
    while (i < length) {
        if (!obj_has(chain, current)) {
            break;
        }
        
        let options = chain[current];
        let next = randomChoice(options);
        arr_push(result, next);
        
        // Update state
        let words = split(current, " ");
        arr_shift(words);
        arr_push(words, next);
        current = arr_join(words, " ");
        
        i = i + 1;
    }
    
    return arr_join(result, " ");
}

// ===== TEXT SUMMARIZATION =====

fn nlp_extractiveSummary(text, numSentences) {
    // Extractive summarization
    let sentences = nlp_tokenizeSentences(text);
    let scores = [];
    let i = 0;
    
    // Score sentences by word frequency
    let allWords = nlp_tokenize(text);
    let freq = nlp_wordFrequency(allWords);
    
    while (i < arr_length(sentences)) {
        let words = nlp_tokenize(sentences[i]);
        let score = arr_reduce(words, fn(sum, word) {
            return sum + (obj_has(freq, word) ? freq[word] : 0);
        }, 0);
        
        arr_push(scores, {sentence: sentences[i], score: score});
        i = i + 1;
    }
    
    // Sort by score
    let sorted = arr_sort(scores, fn(a, b) {
        return b.score - a.score;
    });
    
    // Take top sentences
    let top = take(sorted, numSentences);
    return arr_map(top, fn(item) { return item.sentence; });
}
