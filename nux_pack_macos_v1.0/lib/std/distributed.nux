// Nux Standard Library - Distributed Computing
// Cluster computing, distributed data structures, and consensus algorithms

// ===== DISTRIBUTED SYSTEM =====

fn cluster_create(nodes) {
    // Create distributed cluster
    return {
        nodes: nodes,
        leader: null,
        term: 0,
        state: "follower"
    };
}

fn cluster_addNode(cluster, node) {
    // Add node to cluster
    arr_push(cluster.nodes, node);
}

fn cluster_removeNode(cluster, nodeId) {
    // Remove node from cluster
    cluster.nodes = arr_filter(cluster.nodes, fn(n) {
        return n.id != nodeId;
    });
}

// ===== RAFT CONSENSUS ALGORITHM =====

fn raft_create(nodeId, peers) {
    // Create Raft consensus node
    return {
        id: nodeId,
        peers: peers,
        state: "follower",
        currentTerm: 0,
        votedFor: null,
        log: [],
        commitIndex: 0,
        lastApplied: 0,
        nextIndex: {},
        matchIndex: {}
    };
}

fn raft_requestVote(node, term, candidateId) {
    // Handle vote request
    if (term > node.currentTerm) {
        node.currentTerm = term;
        node.state = "follower";
        node.votedFor = null;
    }
    
    if (node.votedFor == null || node.votedFor == candidateId) {
        node.votedFor = candidateId;
        return {voteGranted: true, term: node.currentTerm};
    }
    
    return {voteGranted: false, term: node.currentTerm};
}

fn raft_appendEntries(node, term, leaderId, entries) {
    // Handle append entries (heartbeat or replication)
    if (term >= node.currentTerm) {
        node.currentTerm = term;
        node.state = "follower";
        
        // Append entries to log
        arr_forEach(entries, fn(entry) {
            arr_push(node.log, entry);
        });
        
        return {success: true, term: node.currentTerm};
    }
    
    return {success: false, term: node.currentTerm};
}

fn raft_startElection(node) {
    // Start leader election
    node.state = "candidate";
    node.currentTerm = node.currentTerm + 1;
    node.votedFor = node.id;
    
    let votes = 1;
    
    // Request votes from peers
    arr_forEach(node.peers, fn(peer) {
        let response = raft_requestVote(peer, node.currentTerm, node.id);
        if (response.voteGranted) {
            votes = votes + 1;
        }
    });
    
    // Check if won election
    if (votes > arr_length(node.peers) / 2) {
        node.state = "leader";
        return true;
    }
    
    return false;
}

// ===== DISTRIBUTED HASH TABLE (DHT) =====

fn dht_create(nodeId, numBuckets) {
    // Create DHT node
    return {
        id: nodeId,
        buckets: arr_fill(numBuckets, []),
        numBuckets: numBuckets
    };
}

fn dht_hash(key, numBuckets) {
    // Hash key to bucket
    let hash = 0;
    let i = 0;
    
    while (i < length(key)) {
        hash = (hash * 31 + charCodeAt(key, i)) % numBuckets;
        i = i + 1;
    }
    
    return hash;
}

fn dht_put(dht, key, value) {
    // Store key-value pair
    let bucket = dht_hash(key, dht.numBuckets);
    arr_push(dht.buckets[bucket], {key: key, value: value});
}

fn dht_get(dht, key) {
    // Retrieve value by key
    let bucket = dht_hash(key, dht.numBuckets);
    let items = dht.buckets[bucket];
    
    let i = 0;
    while (i < arr_length(items)) {
        if (items[i].key == key) {
            return items[i].value;
        }
        i = i + 1;
    }
    
    return null;
}

// ===== MAP-REDUCE =====

fn mapreduce_map(data, mapFn) {
    // Map phase (parallel)
    return arr_map(data, mapFn);
}

fn mapreduce_shuffle(mapped) {
    // Shuffle phase (group by key)
    let groups = {};
    
    arr_forEach(mapped, fn(items) {
        arr_forEach(items, fn(item) {
            let key = item.key;
            if (!obj_has(groups, key)) {
                groups[key] = [];
            }
            arr_push(groups[key], item.value);
        });
    });
    
    return groups;
}

fn mapreduce_reduce(shuffled, reduceFn) {
    // Reduce phase (parallel)
    let result = {};
    
    let keys = obj_keys(shuffled);
    arr_forEach(keys, fn(key) {
        result[key] = reduceFn(key, shuffled[key]);
    });
    
    return result;
}

fn mapreduce_run(data, mapFn, reduceFn) {
    // Run complete MapReduce job
    let mapped = mapreduce_map(data, mapFn);
    let shuffled = mapreduce_shuffle(mapped);
    let reduced = mapreduce_reduce(shuffled, reduceFn);
    return reduced;
}

// ===== DISTRIBUTED QUEUE =====

fn distqueue_create(nodes) {
    // Create distributed queue
    return {
        nodes: nodes,
        items: [],
        replicationFactor: 3
    };
}

fn distqueue_enqueue(queue, item) {
    // Add item to queue (replicated)
    arr_push(queue.items, item);
    
    // Replicate to nodes
    let i = 0;
    while (i < queue.replicationFactor && i < arr_length(queue.nodes)) {
        // Send to node
        // TODO: Implement network send
        i = i + 1;
    }
}

fn distqueue_dequeue(queue) {
    // Remove item from queue
    if (arr_length(queue.items) > 0) {
        return arr_shift(queue.items);
    }
    return null;
}

// ===== DISTRIBUTED LOCK =====

fn distlock_create(lockId, nodes) {
    // Create distributed lock
    return {
        id: lockId,
        nodes: nodes,
        holder: null,
        timestamp: 0
    };
}

fn distlock_acquire(lock, nodeId) {
    // Acquire distributed lock
    if (lock.holder == null) {
        lock.holder = nodeId;
        lock.timestamp = date_now();
        return true;
    }
    
    // Check if lock expired
    let now = date_now();
    if (now - lock.timestamp > 30000) {  // 30 second timeout
        lock.holder = nodeId;
        lock.timestamp = now;
        return true;
    }
    
    return false;
}

fn distlock_release(lock, nodeId) {
    // Release distributed lock
    if (lock.holder == nodeId) {
        lock.holder = null;
        lock.timestamp = 0;
        return true;
    }
    return false;
}

// ===== GOSSIP PROTOCOL =====

fn gossip_create(nodeId) {
    // Create gossip node
    return {
        id: nodeId,
        peers: [],
        data: {},
        version: 0
    };
}

fn gossip_addPeer(node, peer) {
    // Add peer
    arr_push(node.peers, peer);
}

fn gossip_update(node, key, value) {
    // Update local data
    node.data[key] = value;
    node.version = node.version + 1;
}

fn gossip_sync(node) {
    // Sync with random peer
    if (arr_length(node.peers) == 0) {
        return;
    }
    
    let randomPeer = randomChoice(node.peers);
    
    // Send data to peer
    // Receive data from peer
    // Merge data
    
    let keys = obj_keys(randomPeer.data);
    arr_forEach(keys, fn(key) {
        if (!obj_has(node.data, key)) {
            node.data[key] = randomPeer.data[key];
        }
    });
}

// ===== VECTOR CLOCK =====

fn vectorclock_create(nodeId, numNodes) {
    // Create vector clock
    let clock = arr_fill(numNodes, 0);
    return {
        nodeId: nodeId,
        clock: clock
    };
}

fn vectorclock_increment(vc) {
    // Increment local clock
    vc.clock[vc.nodeId] = vc.clock[vc.nodeId] + 1;
}

fn vectorclock_merge(vc1, vc2) {
    // Merge two vector clocks
    let merged = vectorclock_create(vc1.nodeId, arr_length(vc1.clock));
    
    let i = 0;
    while (i < arr_length(vc1.clock)) {
        merged.clock[i] = max(vc1.clock[i], vc2.clock[i]);
        i = i + 1;
    }
    
    return merged;
}

fn vectorclock_compare(vc1, vc2) {
    // Compare vector clocks
    // Returns: -1 (vc1 < vc2), 0 (concurrent), 1 (vc1 > vc2)
    
    let less = false;
    let greater = false;
    let i = 0;
    
    while (i < arr_length(vc1.clock)) {
        if (vc1.clock[i] < vc2.clock[i]) {
            less = true;
        }
        if (vc1.clock[i] > vc2.clock[i]) {
            greater = true;
        }
        i = i + 1;
    }
    
    if (less && !greater) {
        return -1;
    }
    if (greater && !less) {
        return 1;
    }
    return 0;  // Concurrent
}

// ===== CONSISTENT HASHING =====

fn consistenthash_create(numVirtualNodes) {
    // Create consistent hash ring
    return {
        ring: [],
        nodes: {},
        numVirtualNodes: numVirtualNodes
    };
}

fn consistenthash_addNode(ch, nodeId) {
    // Add node to ring
    let i = 0;
    
    while (i < ch.numVirtualNodes) {
        let virtualId = nodeId + "#" + i;
        let hash = crypto_sha256(virtualId);
        
        arr_push(ch.ring, {hash: hash, nodeId: nodeId});
        i = i + 1;
    }
    
    // Sort ring by hash
    ch.ring = arr_sort(ch.ring, fn(a, b) {
        return a.hash < b.hash ? -1 : 1;
    });
    
    ch.nodes[nodeId] = true;
}

fn consistenthash_getNode(ch, key) {
    // Get node for key
    let hash = crypto_sha256(key);
    
    // Binary search for node
    let i = 0;
    while (i < arr_length(ch.ring)) {
        if (ch.ring[i].hash >= hash) {
            return ch.ring[i].nodeId;
        }
        i = i + 1;
    }
    
    // Wrap around
    return ch.ring[0].nodeId;
}

// ===== DISTRIBUTED TRANSACTION (2PC) =====

fn transaction_prepare(participants, txId) {
    // Phase 1: Prepare
    let votes = [];
    
    arr_forEach(participants, fn(participant) {
        let vote = participant.prepare(txId);
        arr_push(votes, vote);
    });
    
    // Check if all voted yes
    let allYes = arr_every(votes, fn(vote) { return vote == "yes"; });
    
    return allYes;
}

fn transaction_commit(participants, txId) {
    // Phase 2: Commit
    arr_forEach(participants, fn(participant) {
        participant.commit(txId);
    });
}

fn transaction_abort(participants, txId) {
    // Phase 2: Abort
    arr_forEach(participants, fn(participant) {
        participant.abort(txId);
    });
}

fn transaction_2pc(participants, txId) {
    // Two-phase commit
    let canCommit = transaction_prepare(participants, txId);
    
    if (canCommit) {
        transaction_commit(participants, txId);
        return "committed";
    } else {
        transaction_abort(participants, txId);
        return "aborted";
    }
}
